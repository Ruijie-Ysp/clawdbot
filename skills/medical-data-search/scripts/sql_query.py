#!/usr/bin/env python3
"""
åŒ»ç–—æ•°æ®æ¹–SQLæŸ¥è¯¢å·¥å…·

åŠŸèƒ½ï¼š
1. æ‰§è¡ŒSQLæŸ¥è¯¢åŒ»ç–—æ•°æ®æ¹–
2. æ”¯æŒè”é‚¦æŸ¥è¯¢å’Œè·¨æ•°æ®æºJOIN
3. æŸ¥è¯¢ç»“æœå¯¼å‡ºå’Œåˆ†æ
4. æ•°æ®ç›®å½•å’Œè¡¨ç»“æ„æŸ¥è¯¢

ä½¿ç”¨ç¤ºä¾‹ï¼š
python sql_query.py --sql "SELECT * FROM iceberg.bronze.ods_documents_parsed LIMIT 10"
python sql_query.py --sql "SELECT title, author FROM iceberg.bronze.ods_documents_parsed WHERE category='ä¸´åºŠæŒ‡å—'"
python sql_query.py --list-catalogs
python sql_query.py --list-tables iceberg bronze
"""

import os
import sys
import json
import argparse
import logging
from typing import List, Dict, Optional, Any
import requests
import pandas as pd
from datetime import datetime
from tabulate import tabulate

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sql_query.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class SQLQueryClient:
    """SQLæŸ¥è¯¢å®¢æˆ·ç«¯"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–SQLæŸ¥è¯¢å®¢æˆ·ç«¯"""
        self.config = self._load_config(config_path)
        self.api_base = self.config.get('api_base', 'http://localhost:48200')
        self.api_key = self.config.get('api_key')
        self.tenant_id = self.config.get('tenant_id', 'default')
        self.session = requests.Session()
        self.session.headers.update({
            'X-API-Key': self.api_key,
            'X-Tenant-ID': self.tenant_id,
            'Content-Type': 'application/json'
        })
        
    def _load_config(self, config_path: Optional[str] = None) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'api_base': 'http://localhost:48200',
            'api_key': None,
            'tenant_id': 'default',
            'query_options': {
                'default_catalog': 'iceberg',
                'timeout': 60,
                'max_rows': 1000
            }
        }
        
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
                logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            except Exception as e:
                logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        env_api_key = os.getenv('MEDICAL_API_KEY')
        if env_api_key and not default_config['api_key']:
            default_config['api_key'] = env_api_key
            
        return default_config
    
    def execute_sql(self, sql: str, catalog: Optional[str] = None) -> Dict:
        """
        æ‰§è¡ŒSQLæŸ¥è¯¢
        
        Args:
            sql: SQLè¯­å¥
            catalog: æ•°æ®ç›®å½•
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        if not catalog:
            catalog = self.config['query_options']['default_catalog']
        
        query_params = {
            'sql': sql,
            'catalog': catalog
        }
        
        try:
            logger.info(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {sql[:100]}...")
            response = self.session.post(
                f"{self.api_base}/api/v1/query/sql",
                json=query_params,
                timeout=self.config['query_options']['timeout']
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('success'):
                data = result.get('data', {})
                logger.info(f"SQLæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {data.get('row_count', 0)} è¡Œæ•°æ®")
                logger.info(f"æŸ¥è¯¢è€—æ—¶: {data.get('took_ms', 0)}ms")
                return data
            else:
                logger.error(f"SQLæŸ¥è¯¢å¤±è´¥: {result.get('message')}")
                return {'error': result.get('message'), 'rows': []}
                
        except requests.exceptions.RequestException as e:
            logger.error(f"SQLæŸ¥è¯¢è¯·æ±‚å¤±è´¥: {e}")
            return {'error': str(e), 'rows': []}
        except Exception as e:
            logger.error(f"SQLæŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return {'error': str(e), 'rows': []}
    
    def list_catalogs(self) -> List[str]:
        """è·å–æ•°æ®ç›®å½•åˆ—è¡¨"""
        try:
            response = self.session.get(
                f"{self.api_base}/api/v1/query/catalogs",
                timeout=10
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('success'):
                return result.get('data', [])
            else:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: {result.get('message')}")
                return []
                
        except Exception as e:
            logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def list_tables(self, catalog: str, schema: str) -> List[Dict]:
        """è·å–æŒ‡å®šç›®å½•å’Œæ¨¡å¼ä¸‹çš„è¡¨åˆ—è¡¨"""
        try:
            response = self.session.get(
                f"{self.api_base}/api/v1/query/tables/{catalog}/{schema}",
                timeout=10
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('success'):
                return result.get('data', [])
            else:
                logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {result.get('message')}")
                return []
                
        except Exception as e:
            logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def query_data_lake(self, schema: str, table: str, **kwargs) -> Dict:
        """
        æŸ¥è¯¢æ•°æ®æ¹–è¡¨
        
        Args:
            schema: æ¨¡å¼åç§°
            table: è¡¨åç§°
            **kwargs: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            params = {
                'schema': schema,
                'table': table,
                'page': kwargs.get('page', 1),
                'size': kwargs.get('size', 20)
            }
            
            # æ·»åŠ å¯é€‰å‚æ•°
            optional_params = [
                'snapshot_id', 'columns', 'title', 'category', 'author',
                'content', 'keywords', 'source', 'file_type', 'date_from',
                'date_to', 'sort_by', 'sort_order'
            ]
            
            for param in optional_params:
                if param in kwargs and kwargs[param] is not None:
                    params[param] = kwargs[param]
            
            response = self.session.get(
                f"{self.api_base}/api/v1/lake/tables/{schema}/{table}/data",
                params=params,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            if result.get('success'):
                return result.get('data', {})
            else:
                logger.error(f"æ•°æ®æ¹–æŸ¥è¯¢å¤±è´¥: {result.get('message')}")
                return {'error': result.get('message'), 'items': []}
                
        except Exception as e:
            logger.error(f"æ•°æ®æ¹–æŸ¥è¯¢å¤±è´¥: {e}")
            return {'error': str(e), 'items': []}
    
    def export_results(self, results: Dict, format: str = 'table', output_path: Optional[str] = None):
        """å¯¼å‡ºæŸ¥è¯¢ç»“æœ"""
        if 'error' in results:
            logger.warning(f"æ— æ³•å¯¼å‡ºé”™è¯¯ç»“æœ: {results['error']}")
            return
        
        rows = results.get('rows', [])
        columns = results.get('columns', [])
        
        if not rows:
            logger.warning("æ²¡æœ‰ç»“æœå¯å¯¼å‡º")
            return
        
        if format.lower() == 'csv':
            df = pd.DataFrame(rows, columns=columns)
            if output_path:
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°CSV: {output_path}")
            else:
                print(df.to_string())
                
        elif format.lower() == 'json':
            export_data = {
                'columns': columns,
                'rows': rows,
                'row_count': results.get('row_count', len(rows)),
                'took_ms': results.get('took_ms', 0)
            }
            
            if output_path:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°JSON: {output_path}")
            else:
                print(json.dumps(export_data, ensure_ascii=False, indent=2))
                
        elif format.lower() == 'table':
            self._print_table(columns, rows, results)
            
        else:
            logger.error(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
    
    def _print_table(self, columns: List[str], rows: List[List], results: Dict):
        """ä»¥è¡¨æ ¼å½¢å¼æ‰“å°ç»“æœ"""
        if not rows:
            print("æ²¡æœ‰æŸ¥è¯¢ç»“æœ")
            return
        
        # ä½¿ç”¨tabulateæ‰“å°ç¾è§‚çš„è¡¨æ ¼
        table_data = []
        for row in rows[:50]:  # é™åˆ¶æ˜¾ç¤ºå‰50è¡Œ
            table_data.append(row)
        
        print(tabulate(table_data, headers=columns, tablefmt='grid'))
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\næ€»è®¡: {results.get('row_count', len(rows))} è¡Œæ•°æ®")
        print(f"æŸ¥è¯¢è€—æ—¶: {results.get('took_ms', 0)}ms")
        
        if len(rows) > 50:
            print(f"ï¼ˆä»…æ˜¾ç¤ºå‰50è¡Œï¼Œå…±{len(rows)}è¡Œï¼‰")
    
    def analyze_query(self, sql: str) -> Dict:
        """åˆ†æSQLæŸ¥è¯¢"""
        analysis = {
            'sql': sql,
            'estimated_cost': 'unknown',
            'suggestions': [],
            'warnings': []
        }
        
        # ç®€å•çš„SQLåˆ†æ
        sql_lower = sql.lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰LIMITå­å¥
        if 'limit' not in sql_lower:
            analysis['warnings'].append('æŸ¥è¯¢æ²¡æœ‰LIMITå­å¥ï¼Œå¯èƒ½è¿”å›å¤§é‡æ•°æ®')
            analysis['suggestions'].append('æ·»åŠ LIMITå­å¥é™åˆ¶è¿”å›è¡Œæ•°')
        
        # æ£€æŸ¥SELECT *
        if 'select *' in sql_lower:
            analysis['suggestions'].append('å»ºè®®æŒ‡å®šå…·ä½“åˆ—åè€Œä¸æ˜¯ä½¿ç”¨SELECT *')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰WHEREæ¡ä»¶
        if 'where' not in sql_lower and 'join' not in sql_lower:
            analysis['warnings'].append('æŸ¥è¯¢æ²¡æœ‰WHEREæ¡ä»¶ï¼Œå¯èƒ½æ‰«æå…¨è¡¨')
        
        return analysis


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='åŒ»ç–—æ•°æ®æ¹–SQLæŸ¥è¯¢å·¥å…·')
    
    # æŸ¥è¯¢å‚æ•°
    parser.add_argument('--sql', help='SQLè¯­å¥')
    parser.add_argument('--catalog', default='iceberg', help='æ•°æ®ç›®å½•')
    
    # æ•°æ®æ¹–æŸ¥è¯¢å‚æ•°
    parser.add_argument('--schema', help='æ¨¡å¼åç§°ï¼ˆæ•°æ®æ¹–æŸ¥è¯¢ï¼‰')
    parser.add_argument('--table', help='è¡¨åç§°ï¼ˆæ•°æ®æ¹–æŸ¥è¯¢ï¼‰')
    parser.add_argument('--page', type=int, default=1, help='é¡µç ')
    parser.add_argument('--size', type=int, default=20, help='æ¯é¡µæ•°é‡')
    parser.add_argument('--snapshot-id', help='å¿«ç…§IDï¼ˆTime Travelï¼‰')
    parser.add_argument('--columns', help='æŸ¥è¯¢åˆ—ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--title', help='æ ‡é¢˜å…³é”®è¯')
    parser.add_argument('--category', help='åˆ†ç±»ç­›é€‰')
    parser.add_argument('--author', help='ä½œè€…ç­›é€‰')
    parser.add_argument('--content', help='å†…å®¹å…³é”®è¯')
    parser.add_argument('--keywords', help='å¤šä¸ªå…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--date-from', help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--date-to', help='ç»“æŸæ—¥æœŸ')
    
    # ä¿¡æ¯æŸ¥è¯¢
    parser.add_argument('--list-catalogs', action='store_true', 
                       help='åˆ—å‡ºæ‰€æœ‰æ•°æ®ç›®å½•')
    parser.add_argument('--list-tables', nargs=2, metavar=('CATALOG', 'SCHEMA'),
                       help='åˆ—å‡ºæŒ‡å®šç›®å½•å’Œæ¨¡å¼ä¸‹çš„è¡¨')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--output', '-o', default='table', 
                       choices=['json', 'csv', 'table'], help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--save', help='ä¿å­˜ç»“æœåˆ°æ–‡ä»¶')
    parser.add_argument('--analyze', action='store_true', 
                       help='åˆ†æSQLæŸ¥è¯¢')
    
    # é…ç½®é€‰é¡¹
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--api-key', help='APIå¯†é’¥')
    parser.add_argument('--api-base', help='APIåŸºç¡€åœ°å€')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = SQLQueryClient(args.config)
    
    # è¦†ç›–é…ç½®
    if args.api_key:
        client.config['api_key'] = args.api_key
    if args.api_base:
        client.config['api_base'] = args.api_base
    
    # å¤„ç†ä¿¡æ¯æŸ¥è¯¢
    if args.list_catalogs:
        catalogs = client.list_catalogs()
        print("=== æ•°æ®ç›®å½•åˆ—è¡¨ ===")
        for catalog in catalogs:
            print(f"  {catalog}")
        sys.exit(0)
    
    if args.list_tables:
        catalog, schema = args.list_tables
        tables = client.list_tables(catalog, schema)
        print(f"=== è¡¨åˆ—è¡¨ ({catalog}.{schema}) ===")
        for table in tables:
            print(f"  {table.get('name', 'æœªçŸ¥')}")
        sys.exit(0)
    
    # æ£€æŸ¥å¿…è¦çš„å‚æ•°
    if not args.sql and not (args.schema and args.table):
        parser.error("éœ€è¦æä¾› --sql å‚æ•°æˆ– --schema å’Œ --table å‚æ•°")
    
    # æ‰§è¡ŒæŸ¥è¯¢
    if args.sql:
        # SQLæŸ¥è¯¢
        if args.analyze:
            analysis = client.analyze_query(args.sql)
            print("=== SQLæŸ¥è¯¢åˆ†æ ===")
            print(f"SQL: {analysis['sql']}")
            
            if analysis['warnings']:
                print("\nè­¦å‘Š:")
                for warning in analysis['warnings']:
                    print(f"  âš ï¸  {warning}")
            
            if analysis['suggestions']:
                print("\nå»ºè®®:")
                for suggestion in analysis['suggestions']:
                    print(f"  ğŸ’¡ {suggestion}")
            
            print("\næ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿ(y/n): ", end='')
            if input().lower() != 'y':
                sys.exit(0)
        
        results = client.execute_sql(args.sql, args.catalog)
        
    else:
        # æ•°æ®æ¹–æŸ¥è¯¢
        query_kwargs = {
            'page': args.page,
            'size': args.size
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        optional_args = [
            'snapshot_id', 'columns', 'title', 'category', 'author',
            'content', 'keywords', 'date_from', 'date_to'
        ]
        
        for arg in optional_args:
            if getattr(args, arg, None):
                query_kwargs[arg] = getattr(args, arg)
        
        # å¤„ç†å…³é”®è¯
        if args.keywords:
            query_kwargs['keywords'] = args.keywords
        
        results = client.query_data_lake(args.schema, args.table, **query_kwargs)
        
        # è½¬æ¢æ•°æ®æ¹–æŸ¥è¯¢ç»“æœä¸ºç»Ÿä¸€æ ¼å¼
        if 'error' not in results:
            items = results.get('items', [])
            if items:
                columns = list(items[0].keys()) if items else []
                rows = [list(item.values()) for item in items]
                results = {
                    'columns': columns,
                    'rows': rows,
                    'row_count': len(items),
                    'total': results.get('total', len(items))
                }
    
    # å¤„ç†ç»“æœ
    if 'error' in results:
        print(f"æŸ¥è¯¢é”™è¯¯: {results['error']}")
        sys.exit(1)
    
    # å¯¼å‡ºç»“æœ
    output_path = args.save or f"sql_query_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{args.output}"
    client.export_results(results, args.output, output_path if args.save else None)


if __name__ == "__main__":
    main()